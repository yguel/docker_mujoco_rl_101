{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d20a17c",
   "metadata": {},
   "source": [
    "\n",
    "# Discovering a Reinforcement Learning API from a Black‑Box Environment (MuJoCo)\n",
    "\n",
    "**Goal:** Without reading the environment’s source code, you will *infer* a standard RL API—\n",
    "similar to Gym/Gymnasium—by probing a MuJoCo environment treated as a **black box**.\n",
    "\n",
    "By the end, you should be able to:\n",
    "- Identify the **state (observation) space** and **action space**.\n",
    "- Use `reset()` to obtain an **initial state** (and `info`) and discuss reproducibility.\n",
    "- Use `step(action)` to obtain **next_state, reward, terminated, truncated, info**.\n",
    "- Understand and check **`terminated` vs `truncated`** (and why both exist).\n",
    "- Log and visualize a **trajectory** and differentiate **state vs next_state**.\n",
    "- Build the canonical training loop with `done = terminated or truncated`.\n",
    "\n",
    "> You’re encouraged to treat the environment as a sealed box. Your tools are prints, shapes,\n",
    "> bounds, and the API surface. Infer what’s going on from *observations*, not internals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup (run locally as needed) ---\n",
    "# If you don't have these installed, uncomment and run:\n",
    "# !pip install -U gymnasium gymnasium[mujoco] mujoco matplotlib pandas\n",
    "\n",
    "import sys, importlib, math, random, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prefer Gymnasium (it exposes terminated/truncated cleanly).\n",
    "try:\n",
    "    gym = importlib.import_module(\"gymnasium\")\n",
    "except ImportError:\n",
    "    # Fallback to classic gym (>=0.26 also has terminated/truncated)\n",
    "    gym = importlib.import_module(\"gym\")\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Numpy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"Matplotlib:\", plt.matplotlib.__version__)\n",
    "print(\"Gym-like module:\", gym.__name__, getattr(gym, \"__version__\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c12f6",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Make a **black‑box** MuJoCo environment\n",
    "\n",
    "We’ll try a few common MuJoCo IDs until one works. Don’t worry about which one—\n",
    "your task is to *infer* its API purely from interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try a list of common MuJoCo env IDs. We'll pick the first that makes successfully.\n",
    "candidate_ids = [\n",
    "    \"Ant-v5\", \"Ant-v4\",\n",
    "    \"HalfCheetah-v5\", \"HalfCheetah-v4\",\n",
    "    \"Hopper-v5\", \"Hopper-v4\",\n",
    "    \"Walker2d-v5\", \"Walker2d-v4\"\n",
    "]\n",
    "\n",
    "env_id = None\n",
    "env = None\n",
    "for cid in candidate_ids:\n",
    "    try:\n",
    "        env = gym.make(cid)  # uses default TimeLimit if provided by the environment\n",
    "        env_id = cid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        env = None\n",
    "\n",
    "if env is None:\n",
    "    raise RuntimeError(\"Could not create a MuJoCo env. Install gymnasium[mujoco] (or gym[mujoco]) and MuJoCo.\")\n",
    "\n",
    "print(\"Using environment:\", env_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac633261",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Element A — **State (Observation) Space**\n",
    "\n",
    "**Question:** What is the shape, dtype, and typical range of the **state** returned by the env?  \n",
    "**Method:** Inspect `env.observation_space`, then actually call `reset()` to see a sample.\n",
    "\n",
    "> Treat the state as a vector (or array). Don’t assume semantics—just record what you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e63ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Peek at the observation (state) space\n",
    "obs_space = env.observation_space\n",
    "print(\"Observation space type:\", type(obs_space).__name__)\n",
    "print(\"Observation space:\", obs_space)\n",
    "\n",
    "# Reset to get an initial observation without fixing the seed (yet)\n",
    "state, info = env.reset()\n",
    "print(\"\\nInitial state shape:\", np.shape(state), \"dtype:\", getattr(state, \"dtype\", type(state)))\n",
    "print(\"Initial info keys:\", list(info.keys()))\n",
    "# A quick numeric summary (if it's a numeric Box)\n",
    "if hasattr(obs_space, \"low\") and hasattr(obs_space, \"high\"):\n",
    "    print(\"Obs low (first 5):\", np.array(obs_space.low).ravel()[:5])\n",
    "    print(\"Obs high(first 5):\", np.array(obs_space.high).ravel()[:5])\n",
    "print(\"Sample state preview (first 8):\", np.array(state).ravel()[:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358fefdf",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Element B — **Action Space**\n",
    "\n",
    "**Question:** What actions does the env accept? Continuous or discrete? What are the bounds?  \n",
    "**Method:** Inspect `env.action_space`. Then sample a random action and attempt one step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d840a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "act_space = env.action_space\n",
    "print(\"Action space type:\", type(act_space).__name__)\n",
    "print(\"Action space:\", act_space)\n",
    "if hasattr(act_space, \"low\") and hasattr(act_space, \"high\"):\n",
    "    print(\"Action low (first 5):\", np.array(act_space.low).ravel()[:5])\n",
    "    print(\"Action high(first 5):\", np.array(act_space.high).ravel()[:5])\n",
    "\n",
    "# Try a single random step to *observe* the API\n",
    "action = act_space.sample()\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"\\n--- Single step probe ---\")\n",
    "print(\"Action shape:\", np.shape(action))\n",
    "print(\"Next state shape:\", np.shape(next_state))\n",
    "print(\"Reward (scalar):\", reward, \"| type:\", type(reward).__name__)\n",
    "print(\"terminated:\", terminated, \"| truncated:\", truncated)\n",
    "print(\"info keys:\", list(info.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756137d3",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Element C — **Reset semantics, seeding, and (maybe) initial state control**\n",
    "\n",
    "Most black‑box envs offer:\n",
    "- `state, info = env.reset()`\n",
    "- Optional `seed=` for reproducibility.\n",
    "- Sometimes `options={...}` to control initial conditions (not guaranteed).\n",
    "\n",
    "**Task:** Show that seeding produces reproducible initial states. Then *attempt* to set a custom\n",
    "initial state via `options` to see if the env supports it. If not, note the limitation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reproducibility via seeds\n",
    "s1, _ = env.reset(seed=123)\n",
    "s2, _ = env.reset(seed=123)\n",
    "s3, _ = env.reset(seed=456)\n",
    "\n",
    "print(\"Reset with seed=123 identical?\", np.allclose(np.asarray(s1), np.asarray(s2)))\n",
    "print(\"Reset seed=123 vs 456 identical?\", np.allclose(np.asarray(s1), np.asarray(s3)))\n",
    "\n",
    "# Attempt: pass custom options (many MuJoCo envs don't implement this; we *probe* to find out)\n",
    "supports_options = True\n",
    "try:\n",
    "    # This is a *probe*: totally arbitrary content to see if options are validated or ignored.\n",
    "    s_custom, info_custom = env.reset(options={\"state\": np.zeros_like(np.asarray(s1))})\n",
    "    print(\"Custom options accepted. New state (first 8):\", np.array(s_custom).ravel()[:8])\n",
    "except TypeError as e:\n",
    "    supports_options = False\n",
    "    print(\"This env does not accept an 'options' argument in reset():\", e)\n",
    "except Exception as e:\n",
    "    supports_options = False\n",
    "    print(\"Tried options in reset(), but encountered:\", repr(e))\n",
    "\n",
    "print(\"Supports custom options in reset()?\", supports_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735ed65",
   "metadata": {},
   "source": [
    "\n",
    "## 5) `terminated` vs `truncated` and the canonical control loop\n",
    "\n",
    "- **`terminated`:** The episode ended for *task-defined* reasons (e.g., the agent fell).\n",
    "- **`truncated`:** The episode ended due to an *external limit* (e.g., time limit reached).\n",
    "- **`done = terminated or truncated`** is the standard loop condition.\n",
    "\n",
    "**Task:** Run a short episode with random actions. Detect and report whether it ended by\n",
    "termination or truncation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_one_episode_random(env, max_env_steps=None, verbose=True):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += float(reward)\n",
    "        steps += 1\n",
    "        if verbose:\n",
    "            print(f\"t={steps:<3} reward={reward: .3f} terminated={terminated} truncated={truncated}\")\n",
    "        if terminated or truncated:\n",
    "            reason = \"terminated\" if terminated else \"truncated\"\n",
    "            if verbose:\n",
    "                print(f\"Episode ended by **{reason}** after {steps} steps; total_reward={total_reward:.2f}\")\n",
    "            break\n",
    "        if max_env_steps is not None and steps >= max_env_steps:\n",
    "            if verbose:\n",
    "                print(\"Stopping early (max_env_steps reached).\")\n",
    "            break\n",
    "        state = next_state\n",
    "    return steps, total_reward\n",
    "\n",
    "_ = run_one_episode_random(env, verbose=False)\n",
    "print(\"Ran a silent random episode to check everything is wired correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7b7d7",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Demonstrate **truncation** explicitly (TimeLimit wrapper)\n",
    "\n",
    "To *force* a `truncated=True` ending, wrap the env with a small `max_episode_steps`.  \n",
    "We’ll run two short episodes to observe both endings in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86faf369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a fresh environment instance for a clean demo:\n",
    "env_trunc = gym.make(env_id)\n",
    "# Wrap to ensure a short time limit so truncation is observable\n",
    "env_trunc = gym.wrappers.TimeLimit(env_trunc, max_episode_steps=25)\n",
    "\n",
    "print(\"Running with an explicit TimeLimit (25 steps) to trigger truncation...\")\n",
    "steps, total_reward = run_one_episode_random(env_trunc, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d65117",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Logging and visualizing a **trajectory**\n",
    "\n",
    "We’ll collect the sequence\n",
    "\\[(s₀, a₀, s₁, a₁, s₂, …)\\]\n",
    "along with rewards and flags, store it, and plot a few signals.\n",
    "\n",
    "> For high‑dimensional states/actions, we’ll show just the first few components for readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958763b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rollout_random(env, max_steps=200):\n",
    "    s, info = env.reset(seed=42)\n",
    "    traj_alt = []  # alternating list: s0, a0, s1, a1, ...\n",
    "    rows = []      # tabular view\n",
    "\n",
    "    terminated = truncated = False\n",
    "    t = 0\n",
    "    while not (terminated or truncated) and t < max_steps:\n",
    "        a = env.action_space.sample()\n",
    "        s_next, r, terminated, truncated, info = env.step(a)\n",
    "\n",
    "        traj_alt.append(np.asarray(s))\n",
    "        traj_alt.append(np.asarray(a))\n",
    "\n",
    "        rows.append({\n",
    "            \"t\": t,\n",
    "            \"reward\": float(r),\n",
    "            \"terminated\": bool(terminated),\n",
    "            \"truncated\": bool(truncated),\n",
    "            # show only first few dims for compactness\n",
    "            \"s0\": float(np.asarray(s).ravel()[0]) if np.asarray(s).size > 0 else np.nan,\n",
    "            \"s1\": float(np.asarray(s).ravel()[1]) if np.asarray(s).size > 1 else np.nan,\n",
    "            \"a0\": float(np.asarray(a).ravel()[0]) if np.asarray(a).size > 0 else np.nan,\n",
    "            \"a1\": float(np.asarray(a).ravel()[1]) if np.asarray(a).size > 1 else np.nan,\n",
    "        })\n",
    "        s = s_next\n",
    "        t += 1\n",
    "\n",
    "    # Append the final state at the end to complete (..., s_T)\n",
    "    traj_alt.append(np.asarray(s))\n",
    "    return traj_alt, pd.DataFrame(rows)\n",
    "\n",
    "traj_alt, traj_df = rollout_random(env_trunc, max_steps=100)\n",
    "print(f\"Alternating list length = {len(traj_alt)} (should be 2*T + 1)\")\n",
    "print(\"First 3 entries types:\", type(traj_alt[0]).__name__, type(traj_alt[1]).__name__, type(traj_alt[2]).__name__)\n",
    "print(traj_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the DataFrame as a table (interactive if supported in your environment)\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"Trajectory (first components only)\", traj_df)\n",
    "except Exception as e:\n",
    "    # Fallback: just display\n",
    "    traj_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1db7eb",
   "metadata": {},
   "source": [
    "\n",
    "### Plot a couple of simple signals\n",
    "\n",
    "- Reward per step  \n",
    "- One state component over time (if available)\n",
    "\n",
    "> Note: We use plain Matplotlib (no seaborn), single plot per figure, and default colors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8804e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reward per step\n",
    "plt.figure()\n",
    "plt.plot(traj_df[\"t\"], traj_df[\"reward\"])\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.title(\"Reward per step\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First state component (if present)\n",
    "if \"s0\" in traj_df:\n",
    "    plt.figure()\n",
    "    plt.plot(traj_df[\"t\"], traj_df[\"s0\"])\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"state[0]\")\n",
    "    plt.title(\"First state component over time\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e9c67",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Reconstructing the **Gym‑style API** (what you should have *inferred*)\n",
    "\n",
    "- **Reset:**  \n",
    "  `state, info = env.reset(seed=..., options=...)`  \n",
    "  Returns initial **state** and an **info** dict. `seed` controls reproducibility.  \n",
    "  `options` may allow setting initial conditions (if implemented).\n",
    "\n",
    "- **Step:**  \n",
    "  `next_state, reward, terminated, truncated, info = env.step(action)`  \n",
    "  - `next_state`: same space as `state` (shape/dtype inferred from `observation_space`).  \n",
    "  - `reward`: scalar float.  \n",
    "  - `terminated`: task success/failure or natural terminal condition.  \n",
    "  - `truncated`: episode cut short by external limit (e.g., `TimeLimit`).  \n",
    "  - **Check `done = terminated or truncated`.**\n",
    "\n",
    "- **Spaces:**  \n",
    "  - `env.observation_space` (e.g., `Box(low, high, shape, dtype)`)  \n",
    "  - `env.action_space` (e.g., `Box` for continuous torques).  \n",
    "  You can `sample()` from spaces to probe shapes and valid ranges.\n",
    "\n",
    "- **Trajectory:**  \n",
    "  Log tuples `(s_t, a_t, r_t, s_{t+1}, terminated, truncated, info)` or the alternating list  \n",
    "  `(s0, a0, s1, a1, ..., s_T)`. Visualize to build intuition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76abde",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Exercises (for you to explore)\n",
    "\n",
    "1. **Bounds sanity check:** Sample 1,000 random actions and verify they always lie within `action_space` bounds.\n",
    "2. **Seed reproducibility:** Show that with a fixed `seed`, both the initial state and the first *k* steps (under a fixed action sequence) are reproducible.\n",
    "3. **Terminated vs truncated:** Increase the `TimeLimit` to a large number and try to cause a `terminated=True` naturally (e.g., make the agent fall). What qualitative differences do you observe?\n",
    "4. **Custom reset options:** If `options` are not supported, wrap the env in your own `ResettableWrapper` that saves the last state and restores it on reset (advanced; requires env-specific state setters). Discuss why true black‑box envs may *not* permit arbitrary state setting.\n",
    "5. **Policy stub:** Replace the random policy with a zero action (or small PD controller) and compare trajectories.\n",
    "6. **Replay:** Store the trajectory and replay it (e.g., with a video wrapper) to visually connect `terminated` vs `truncated` with behavior. (Requires ffmpeg; optional.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: close envs when done\n",
    "try:\n",
    "    env.close()\n",
    "    env_trunc.close()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
